# @package _global_  # This line indicates a global package declaration
# 
# Model  # Section defining the model architecture
model:
  _target_: sam2.modeling.sam2_base.SAM2Base  # Specifies the base model class
  image_encoder:  # Configuration for the image encoder
    _target_: sam2.modeling.backbones.image_encoder.ImageEncoder  # Specifies the image encoder class
    scalp: 1  # Scaling factor for the image encoder
    trunk:  # Configuration for the backbone of the image encoder
      _target_: sam2.modeling.backbones.hieradet.Hiera  # Specifies the backbone class
      embed_dim: 144  # Dimensionality of the feature embeddings
      num_heads: 2  # Number of attention heads in the backbone
      stages: [2, 6, 36, 4]  # Stages of the backbone network
      global_att_blocks: [23, 33, 43]  # Global attention blocks in the backbone
      window_pos_embed_bkg_spatial_size: [7, 7]  # Spatial size for window position embedding
      window_spec: [8, 4, 16, 8]  # Window specifications for the backbone
    neck:  # Configuration for the feature pyramid network (FPN)
      _target_: sam2.modeling.backbones.image_encoder.FpnNeck  # Specifies the FPN class
      position_encoding:  # Positional encoding for the FPN
        _target_: sam2.modeling.position_encoding.PositionEmbeddingSine  # Specifies the positional encoding class
        num_pos_feats: 256  # Number of positional features
        normalize: true  # Normalize positional features
        scale: null  # Scaling factor for positional features
        temperature: 10000  # Temperature for positional encoding
      d_model: 256  # Dimensionality of the FPN features
      backbone_channel_list: [1152, 576, 288, 144]  # Channel list for the backbone
      fpn_top_down_levels: [2, 3]  # Levels of the FPN that directly use backbone features
      fpn_interp_model: nearest  # Interpolation method for upsampling in the FPN

  memory_attention:  # Configuration for the memory attention module
    _target_: sam2.modeling.memory_attention.MemoryAttention  # Specifies the memory attention class
    d_model: 256  # Dimensionality of features in the memory attention
    pos_enc_at_input: true  # Apply positional encoding at the input
    layer:  # Configuration for the layer within the memory attention
      _target_: sam2.modeling.memory_attention.MemoryAttentionLayer  # Specifies the layer class
      activation: relu  # Activation function used in the layer
      dim_feedforward: 2048  # Dimensionality of the feedforward network
      dropout: 0.1  # Dropout rate for the layer
      pos_enc_at_attn: false  # Apply positional encoding during attention
      self_attention:  # Configuration for the self-attention mechanism
        _target_: sam2.modeling.sam.transformer.RoPEAttention  # Specifies the self-attention class
        rope_theta: 10000.0  # Theta value for Rotary Position Embedding (RoPE)
        feat_sizes: [32, 32]  # Feature sizes for RoPE
        embedding_dim: 256  # Embedding dimension for RoPE
        num_heads: 1  # Number of attention heads
        downsample_rate: 1  # Downsampling rate
        dropout: 0.1  # Dropout rate for self-attention
      d_model: 256  # Dimensionality of the features in the layer
      pos_enc_at_cross_attn_keys: true  # Apply positional encoding for cross-attention keys
      pos_enc_at_cross_attn_queries: false  # Apply positional encoding for cross-attention queries
      cross_attention:  # Configuration for the cross-attention mechanism
        _target_: sam2.modeling.sam.transformer.RoPEAttention  # Specifies the cross-attention class
        rope_theta: 10000.0  # Theta value for RoPE in cross-attention
        feat_sizes: [32, 32]  # Feature sizes for RoPE in cross-attention
        rope_k_repeat: True  # Repeat keys for RoPE
        embedding_dim: 256  # Embedding dimension for RoPE in cross-attention
        num_heads: 1  # Number of attention heads in cross-attention
        downsample_rate: 1  # Downsampling rate in cross-attention
        dropout: 0.1  # Dropout rate for cross-attention
        kv_in_dim: 64  # Dimensionality of key and value inputs for cross-attention
    num_layers: 4  # Number of layers in the memory attention module

  memory_encoder:  # Configuration for the memory encoder
    _target_: sam2.modeling.memory_encoder.MemoryEncoder  # Specifies the memory encoder class
    out_dim: 64  # Dimensionality of the output from the memory encoder
    position_encoding:  # Positional encoding for the memory encoder
      _target_: sam2.modeling.position_encoding.PositionEmbeddingSine  # Specifies the positional encoding class
      num_pos_feats: 64  # Number of positional features
      normalize: true  # Normalize positional features
      scale: null  # Scaling factor for positional features
      temperature: 10000  # Temperature for positional encoding
    mask_downsampler:  # Configuration for the mask downsampler
      _target_: sam2.modeling.memory_encoder.MaskDownSampler  # Specifies the mask downsampler class
      kernel_size: 3  # Kernel size for downsampling
      stride: 2  # Stride for downsampling
      padding: 1  # Padding for downsampling
    fuser:  # Configuration for the feature fuser
      _target_: sam2.modeling.memory_encoder.Fuser  # Specifies the feature fuser class
      layer:  # Configuration for the layer within the fuser
        _target_: sam2.modeling.memory_encoder.CXBlock  # Specifies the layer class
        dim: 256  # Dimensionality of the layer
        kernel_size: 7  # Kernel size for the layer
        padding: 3  # Padding for the layer
        layer_scale_init_value: 1e-6  # Initial value for layer scaling
        use_dwconv: True  # Use depthwise convolution
      num_layers: 2  # Number of layers in the fuser

  num_maskmem: 7  # Number of memory slots for storing masks
  image_size: 1024  # Expected input image size
  # apply scaled sigmoid on mask logits for memory encoder, and directly feed input mask as output mask
  sigmoid_scale_for_mem_enc: 20.0  # Scaling factor for sigmoid in the memory encoder
  sigmoid_bias_for_mem_enc: -10.0  # Bias term for sigmoid in the memory encoder
  use_mask_input_as_output_without_sam: true  # Use input mask directly as output without SAM processing
  # Memory
  directly_add_no_mem_embed: true  # Add non-memory embeddings directly
  # use high-resolution feature map in the SAM mask decoder
  use_high_res_features_in_sam: true  # Use high-resolution features in SAM
  # output 3 masks on the first click on initial conditioning frames
  multimask_output_in_sam: true  # Output multiple masks in SAM
  # SAM heads
  iou_prediction_use_sigmoid: True  # Use sigmoid for IoU prediction
  # cross-attend to object pointers from other frames (based on SAM output tokens) in the encoder
  use_obj_ptrs_in_encoder: true  # Use object pointers in the encoder
  add_tpos_enc_to_obj_ptrs: false  # Add temporal positional encoding to object pointers
  only_obj_ptrs_in_the_past_for_eval: true  # Only use past object pointers for evaluation
  # object occlusion prediction
  pred_obj_scores: true  # Predict object scores
  pred_obj_scores_mlp: true  # Use MLP for object score prediction
  fixed_no_obj_ptr: true  # Use a fixed object pointer for "no object"
  # multimask tracking settings
  multimask_output_for_tracking: true  # Output multiple masks for tracking
  use_multimask_token_for_obj_ptr: true  # Use multi-mask tokens for object pointers
  multimask_min_pt_num: 0  # Minimum number of points for multi-mask
  multimask_max_pt_num: 1  # Maximum number of points for multi-mask
  use_mlp_for_obj_ptr_proj: true  # Use MLP for object pointer projection
  # Compilation flag
  compile_image_encoder: False  # Flag for compiling the image encoder
