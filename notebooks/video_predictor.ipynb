{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59cc33c6-a783-4cc1-a7c1-ae7fb1553392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 100 frames to /lisc/scratch/neurobiology/zimmer/schaar/Behavior/High_Res_Population/110620024/test_SAM2/2024-06-10_14-58-26_trainingsdata_clean3/2024-06-10_14-58-26_trainingsdata_clean3_track_0/output/frame_directory (from total 6000 frames)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|███████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.01it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 200.75 MiB is free. Process 1557306 has 7.95 GiB memory in use. Including non-PyTorch memory, this process has 6.42 GiB memory in use. Of the allocated memory 5.93 GiB is allocated by PyTorch, and 372.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 130\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m video_segments\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 130\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[17], line 120\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m predictor \u001b[38;5;241m=\u001b[39m build_sam2_video_predictor(model_cfg, sam2_checkpoint, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Generate masklet across the video based on the specified coordinate and frame\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m video_segments \u001b[38;5;241m=\u001b[39m generate_masklet(predictor, frames_dir, coordinate, frame_number)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMasklet generated across the video.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Plot the masklet\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 53\u001b[0m, in \u001b[0;36mgenerate_masklet\u001b[0;34m(predictor, frames_dir, coordinate, frame_number)\u001b[0m\n\u001b[1;32m     50\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)  \u001b[38;5;66;03m# 1 for positive click\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Initialize inference state with the frames directory\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m inference_state \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39minit_state(video_path\u001b[38;5;241m=\u001b[39mframes_dir)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Add the point\u001b[39;00m\n\u001b[1;32m     56\u001b[0m _, out_obj_ids, out_mask_logits \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39madd_new_points_or_box(\n\u001b[1;32m     57\u001b[0m     inference_state\u001b[38;5;241m=\u001b[39minference_state,\n\u001b[1;32m     58\u001b[0m     frame_idx\u001b[38;5;241m=\u001b[39mframe_number,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m     62\u001b[0m )\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/.conda/envs/SAM2_shared/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/schaar/code/github/segment-anything-2/sam2/sam2_video_predictor.py:106\u001b[0m, in \u001b[0;36mSAM2VideoPredictor.init_state\u001b[0;34m(self, video_path, offload_video_to_cpu, offload_state_to_cpu, async_loading_frames)\u001b[0m\n\u001b[1;32m    104\u001b[0m inference_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes_already_tracked\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Warm up the visual backbone and cache the image feature on frame 0\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_image_feature(inference_state, frame_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inference_state\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/schaar/code/github/segment-anything-2/sam2/sam2_video_predictor.py:801\u001b[0m, in \u001b[0;36mSAM2VideoPredictor._get_image_feature\u001b[0;34m(self, inference_state, frame_idx, batch_size)\u001b[0m\n\u001b[1;32m    799\u001b[0m device \u001b[38;5;241m=\u001b[39m inference_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    800\u001b[0m image \u001b[38;5;241m=\u001b[39m inference_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m][frame_idx]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 801\u001b[0m backbone_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_image(image)\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Cache the most recent frame's feature (for repeated interactions with\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# a frame; we can use an LRU cache for more frames in the future).\u001b[39;00m\n\u001b[1;32m    804\u001b[0m inference_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcached_features\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {frame_idx: (image, backbone_out)}\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/schaar/code/github/segment-anything-2/sam2/modeling/sam2_base.py:465\u001b[0m, in \u001b[0;36mSAM2Base.forward_image\u001b[0;34m(self, img_batch)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_batch: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the image feature on the input batch.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m     backbone_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_encoder(img_batch)\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_high_res_features_in_sam:\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;66;03m# precompute projected level 0 and level 1 features in SAM decoder\u001b[39;00m\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;66;03m# to avoid running it again on every SAM click\u001b[39;00m\n\u001b[1;32m    469\u001b[0m         backbone_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone_fpn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msam_mask_decoder\u001b[38;5;241m.\u001b[39mconv_s0(\n\u001b[1;32m    470\u001b[0m             backbone_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone_fpn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    471\u001b[0m         )\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/.conda/envs/SAM2_shared/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/.conda/envs/SAM2_shared/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/schaar/code/github/segment-anything-2/sam2/modeling/backbones/image_encoder.py:31\u001b[0m, in \u001b[0;36mImageEncoder.forward\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Forward through backbone\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     features, pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrunk(sample))\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# Discard the lowest resolution features\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         features, pos \u001b[38;5;241m=\u001b[39m features[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp], pos[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp]\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/.conda/envs/SAM2_shared/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/.conda/envs/SAM2_shared/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/schaar/code/github/segment-anything-2/sam2/modeling/backbones/hieradet.py:284\u001b[0m, in \u001b[0;36mHiera.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    282\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m--> 284\u001b[0m     x \u001b[38;5;241m=\u001b[39m blk(x)\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_ends[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    286\u001b[0m         i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_ends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_interm_layers\n\u001b[1;32m    287\u001b[0m     ):\n\u001b[1;32m    288\u001b[0m         feats \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/.conda/envs/SAM2_shared/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/.conda/envs/SAM2_shared/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/schaar/code/github/segment-anything-2/sam2/modeling/backbones/hieradet.py:147\u001b[0m, in \u001b[0;36mMultiScaleBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, window_size)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Window Attention + Q Pooling (if stage change)\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_stride:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Shapes have changed due to Q pooling\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_stride[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/.conda/envs/SAM2_shared/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/.conda/envs/SAM2_shared/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/lisc/scratch/neurobiology/zimmer/schaar/code/github/segment-anything-2/sam2/modeling/backbones/hieradet.py:68\u001b[0m, in \u001b[0;36mMultiScaleAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m     q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mreshape(B, H \u001b[38;5;241m*\u001b[39m W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Torch's SDPA expects [B, nheads, H*W, C] so we transpose\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m     69\u001b[0m     q\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     70\u001b[0m     k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     71\u001b[0m     v\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Transpose back\u001b[39;00m\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 200.75 MiB is free. Process 1557306 has 7.95 GiB memory in use. Including non-PyTorch memory, this process has 6.42 GiB memory in use. Of the allocated memory 5.93 GiB is allocated by PyTorch, and 372.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def create_frames_directory(video_path, max_frames=100):\n",
    "    # Get the directory of the video\n",
    "    video_dir = os.path.dirname(video_path)\n",
    "    \n",
    "    # Create a new directory named 'frame_directory' in the same location as the video\n",
    "    frames_dir = os.path.join(video_dir, 'frame_directory')\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "    \n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not video.isOpened():\n",
    "        raise ValueError(f\"Error opening video file: {video_path}\")\n",
    "    \n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_step = max(1, total_frames // max_frames)\n",
    "    \n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "    while saved_count < max_frames:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % frame_step == 0:\n",
    "            # Save the frame as an image file\n",
    "            frame_filename = os.path.join(frames_dir, f\"{saved_count:05d}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            saved_count += 1\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    video.release()\n",
    "    \n",
    "    print(f\"Extracted {saved_count} frames to {frames_dir} (from total {total_frames} frames)\")\n",
    "    \n",
    "    # Return the path to the directory containing the frames\n",
    "    return frames_dir\n",
    "\n",
    "def generate_masklet(predictor, frames_dir, coordinate, frame_number):\n",
    "    x, y = coordinate\n",
    "    points = np.array([[x, y]], dtype=np.float32)\n",
    "    labels = np.array([1], dtype=np.int32)  # 1 for positive click\n",
    "\n",
    "    # Initialize inference state with the frames directory\n",
    "    inference_state = predictor.init_state(video_path=frames_dir)\n",
    "\n",
    "    # Add the point\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=frame_number,\n",
    "        obj_id=0,  # Assuming we're working with the first object\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    # Propagate through the video and collect results\n",
    "    video_segments = {}\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "        video_segments[out_frame_idx] = {\n",
    "            out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "            for i, out_obj_id in enumerate(out_obj_ids)\n",
    "        }\n",
    "\n",
    "    return video_segments\n",
    "\n",
    "def plot_masklet(frames_dir, video_segments, num_frames=5):\n",
    "    frame_files = sorted(os.listdir(frames_dir))\n",
    "    num_frames = min(num_frames, len(frame_files))\n",
    "    fig, axs = plt.subplots(2, num_frames, figsize=(5*num_frames, 10))\n",
    "    \n",
    "    frame_indices = sorted(video_segments.keys())\n",
    "    selected_indices = [frame_indices[i * len(frame_indices) // num_frames] for i in range(num_frames)]\n",
    "    \n",
    "    for i, frame_idx in enumerate(selected_indices):\n",
    "        # Load and plot original frame\n",
    "        frame_path = os.path.join(frames_dir, frame_files[frame_idx])\n",
    "        frame = cv2.imread(frame_path)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        axs[0, i].imshow(frame_rgb)\n",
    "        axs[0, i].set_title(f\"Frame {frame_idx}\")\n",
    "        axs[0, i].axis('off')\n",
    "        \n",
    "        # Plot frame with masklet overlay\n",
    "        axs[1, i].imshow(frame_rgb)\n",
    "        mask = next(iter(video_segments[frame_idx].values()))  # Get the first (and likely only) mask\n",
    "        axs[1, i].imshow(mask, alpha=0.7, cmap='jet')\n",
    "        axs[1, i].set_title(f\"Masklet {frame_idx}\")\n",
    "        axs[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Default parameters\n",
    "    sam2_checkpoint = \"../checkpoints/sam2_hiera_large.pt\"\n",
    "    model_cfg = \"sam2_hiera_l.yaml\"\n",
    "    coordinate = (210, 350)\n",
    "    frame_number = 10\n",
    "\n",
    "    video_path = \"/lisc/scratch/neurobiology/zimmer/schaar/Behavior/High_Res_Population/110620024/test_SAM2/2024-06-10_14-58-26_trainingsdata_clean3/2024-06-10_14-58-26_trainingsdata_clean3_track_0/output/track.avi\"\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Extract frames from video (limit to 100 frames)\n",
    "    frames_dir = create_frames_directory(video_path, max_frames=100)\n",
    "\n",
    "    # Load the SAM2 model\n",
    "    predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
    "    \n",
    "    # Generate masklet across the video based on the specified coordinate and frame\n",
    "    video_segments = generate_masklet(predictor, frames_dir, coordinate, frame_number)\n",
    "    \n",
    "    print(\"Masklet generated across the video.\")\n",
    "    \n",
    "    # Plot the masklet\n",
    "    plot_masklet(frames_dir, video_segments)\n",
    "    \n",
    "    return video_segments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62c275-fe73-4d74-96c3-43af44f9f12b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
